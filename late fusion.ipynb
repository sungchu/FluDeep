{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a482e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# with open('../structure_data_remove_Lymphocyte_Troponin/LightGBM_withoutCXR_v2_AUC0.8103.pickle', 'rb') as f:\n",
    "#     LightGBM = pickle.load(f)\n",
    "# with open('../structure_data_remove_Lymphocyte_Troponin/CatBoost_AUC0.7812.pickle', 'rb') as f:\n",
    "#     CatBoost = pickle.load(f)\n",
    "# with open('../structure_data_remove_Lymphocyte_Troponin/XGB_AUC0.8457.pickle', 'rb') as f:\n",
    "#     XGBoostClassifier = pickle.load(f)\n",
    "# with open('../structure_data_remove_Lymphocyte_Troponin/RF_AUC0.8295.pickle', 'rb') as f:\n",
    "#     RandomForest = pickle.load(f)\n",
    "# xception = load_model('../CXR/xception_mse0.9084_val_mse1.1377.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667ce5b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../CXR/train_test_split.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21880/1953502801.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_ID_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../CXR/train_test_split.xlsx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_ID_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../CXR/train_test_split.xlsx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_ID_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ID_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_ID_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ID_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1374\u001b[0m                 \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m                 ext = inspect_excel_format(\n\u001b[0m\u001b[0;32m   1377\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m                 )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m   1251\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m     ) as handle:\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../CXR/train_test_split.xlsx'"
     ]
    }
   ],
   "source": [
    "train_ID_score = pd.read_excel(\"../CXR/train_test_split.xlsx\", sheet_name = 'train')\n",
    "test_ID_score = pd.read_excel(\"../CXR/train_test_split.xlsx\", sheet_name = 'test')\n",
    "\n",
    "train_ID_list = [int(i.split('_')[0]) for i in list(train_ID_score.ID)]\n",
    "test_ID_list = [int(i.split('_')[0]) for i in list(test_ID_score.ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e84868",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NTUH dataset\n",
    "dataset = pd.read_csv('../concat_influ8_severity_score.csv')\n",
    "dataset = dataset[['ID', 'Gender', 'Age', 'BMI', 'Diastolic_blood_pressure', 'Heart_rate', \n",
    "                   'Respiratory_rate', 'Creatinine', 'CRP', 'Hemoglobin', 'Bicarbonate', \n",
    "                   'PCO2', 'pH', 'Platelet', 'PO2',  'Blood_urea_nitrogen', \n",
    "                   'Lactic_acid', 'INR', 'Glucose', 'Hematocrit', 'severity_score', 'Survival_30']]\n",
    "train = dataset.loc[dataset['ID'].isin(train_ID_list)]  #534筆(佔69.8%)\n",
    "train = train.drop('ID', axis = 1)\n",
    "test = dataset.loc[dataset['ID'].isin(test_ID_list)]  #231筆(佔30.2%)\n",
    "test = test.drop('ID', axis = 1)\n",
    "\n",
    "### random sampling do not have good results\n",
    "# X_train = train.drop(['severity_score', 'Survival_30'], axis = True)\n",
    "# X_test = test.drop(['severity_score', 'Survival_30'], axis = True)\n",
    "# y_train = train[['Survival_30']]\n",
    "# y_test = test[['Survival_30']]\n",
    "\n",
    "### Resampling\n",
    "count_class_0, count_class_1 = train.Survival_30.value_counts()\n",
    "df_class_0 = train[train['Survival_30'] == 0]\n",
    "df_class_1 = train[train['Survival_30'] == 1]\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "### validation ###\n",
    "X_train = df_test_over.drop(['severity_score', 'Survival_30'], axis = 1)\n",
    "y_train = df_test_over[['Survival_30']]\n",
    "X_test = test.drop(['severity_score', 'Survival_30'], axis = 1)\n",
    "y_test = test[['Survival_30']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Italian dataset\n",
    "external_dataset = pd.read_csv(\"../concat_italy_severity_score.csv\")\n",
    "# ##\n",
    "# external_dataset = external_dataset[external_dataset.isnull().sum(axis=1) <= 5]\n",
    "# ##\n",
    "external_dataset = external_dataset[['Gender', 'Age', 'BMI', 'DBP (mmHg)', 'Heart rate ', 'RR', \n",
    "                                  'Creatinine ', 'CRP', 'Hb', 'HCO3-', 'pCo2', 'Ph',\n",
    "                                  'PLT', 'pO2', 'BUN ', 'Lactate', 'INR ', 'Glycemia', \n",
    "                                  'HCT', 'severity_score','30 days mortality (0 alive, 1 death)']]\n",
    "external_dataset.columns = ['Gender', 'Age', 'BMI', 'Diastolic_blood_pressure', 'Heart_rate', \n",
    "                   'Respiratory_rate', 'Creatinine', 'CRP', 'Hemoglobin', 'Bicarbonate', \n",
    "                   'PCO2', 'pH', 'Platelet', 'PO2',  'Blood_urea_nitrogen', \n",
    "                   'Lactic_acid', 'INR', 'Glucose', 'Hematocrit', 'severity_score', 'Survival_30']\n",
    "external_dataset['Gender'] = external_dataset['Gender'].replace('F',0).replace('M',1)\n",
    "external_dataset['BMI'] = external_dataset['BMI'].replace(\"Normal\", \"18.5\")\n",
    "external_dataset['BMI'] = external_dataset['BMI'].replace(\"Overweight\", \"21.75\")\n",
    "external_dataset['BMI'] = external_dataset['BMI'].replace(\"Obese \", \"27.5\")\n",
    "external_dataset['BMI'] = external_dataset['BMI'].replace(\"Underweight\", \"30\")\n",
    "external_dataset['Gender'] = pd.to_numeric(external_dataset['Gender'], errors='coerce')\n",
    "external_dataset['BMI'] = pd.to_numeric(external_dataset['BMI'], errors='coerce')\n",
    "\n",
    "X_external = external_dataset.drop(['severity_score', 'Survival_30'], axis = 1)\n",
    "y_external = external_dataset[['Survival_30']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_dataset = pd.read_csv(\"../concat_italy_severity_score.csv\")\n",
    "# ##\n",
    "# external_dataset = external_dataset[external_dataset.isnull().sum(axis=1) <= 5]\n",
    "# ##\n",
    "external_dataset = external_dataset[['Gender', 'Age', 'BMI', 'DBP (mmHg)', 'Heart rate ', 'RR', \n",
    "                                  'Creatinine ', 'CRP', 'Hb', 'HCO3-', 'pCo2', 'Ph',\n",
    "                                  'PLT', 'pO2', 'BUN ', 'Lactate', 'INR ', 'Glycemia', \n",
    "                                  'HCT', 'severity_score','30 days mortality (0 alive, 1 death)']]\n",
    "external_dataset['Gender'] = external_dataset['Gender'].replace('F',0).replace('M',1)\n",
    "external_dataset['BMI'] = external_dataset['BMI'].replace(\"Normal\", \"18.5\")\n",
    "external_dataset['BMI'] = external_dataset['BMI'].replace(\"Overweight\", \"21.75\")\n",
    "external_dataset['BMI'] = external_dataset['BMI'].replace(\"Obese \", \"27.5\")\n",
    "external_dataset['BMI'] = external_dataset['BMI'].replace(\"Underweight\", \"30\")\n",
    "external_dataset['Gender'] = pd.to_numeric(external_dataset['Gender'], errors='coerce')\n",
    "external_dataset['BMI'] = pd.to_numeric(external_dataset['BMI'], errors='coerce')\n",
    "external_dataset = external_dataset.fillna(round(external_dataset.mean(), 2))\n",
    "\n",
    "external_dataset.columns = ['Gender', 'Age', 'BMI', 'Diastolic_blood_pressure', 'Heart_rate', \n",
    "                   'Respiratory_rate', 'Creatinine', 'CRP', 'Hemoglobin', 'Bicarbonate', \n",
    "                   'PCO2', 'pH', 'Platelet', 'PO2',  'Blood_urea_nitrogen', \n",
    "                   'Lactic_acid', 'INR', 'Glucose', 'Hematocrit', 'severity_score', 'Survival_30']\n",
    "\n",
    "X_external_RF = external_dataset.drop(['severity_score', 'Survival_30'], axis = 1)\n",
    "y_external_RF = external_dataset[['Survival_30']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d87e2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "xception_pred_train = df_test_over[['severity_score']]\n",
    "RandomForest_predict_proba_train = RandomForest.predict_proba(X_train)[:, 1]\n",
    "\n",
    "\n",
    "# test\n",
    "xception_pred_test = test[['severity_score']]\n",
    "RandomForest_predict_proba_test = RandomForest.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# external\n",
    "xception_pred_external = external_dataset[['severity_score']]\n",
    "RandomForest_predict_proba_external = RandomForest.predict_proba(X_external_RF)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf0bd415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_train[\"LightGBM\"] = LightGBM_predict_proba_train\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_test[\"LightGBM\"] = LightGBM_predict_proba_test\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_external[\"LightGBM\"] = LightGBM_predict_proba_external\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_train[\"CatBoost\"] = CatBoost_predict_proba_train\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_test[\"CatBoost\"] = CatBoost_predict_proba_test\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_external[\"CatBoost\"] = CatBoost_predict_proba_external\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_train[\"XGBoostClassifier\"] = XGBoostClassifier_predict_proba_train\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_test[\"XGBoostClassifier\"] = XGBoostClassifier_predict_proba_test\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_external[\"XGBoostClassifier\"] = XGBoostClassifier_predict_proba_external\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_train[\"RandomForest\"] = RandomForest_predict_proba_train\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_test[\"RandomForest\"] = RandomForest_predict_proba_test\n",
      "D:\\Users\\122156\\AppData\\Local\\Temp/ipykernel_20136/2565043229.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xception_pred_external[\"RandomForest\"] = RandomForest_predict_proba_external\n"
     ]
    }
   ],
   "source": [
    "xception_pred_train[\"RandomForest\"] = RandomForest_predict_proba_train\n",
    "xception_pred_test[\"RandomForest\"] = RandomForest_predict_proba_test\n",
    "xception_pred_external[\"RandomForest\"] = RandomForest_predict_proba_external"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e759d",
   "metadata": {},
   "source": [
    "# best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef15cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "model_file_name='./Late_Fusion_RF_lr_AUC0.8371.pickle'\n",
    "with open(model_file_name, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39aded0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "X_train = xception_pred_train[[\"severity_score\", \"RandomForest\"]]\n",
    "X_test = xception_pred_test[[\"severity_score\", \"RandomForest\"]]\n",
    "X_external = xception_pred_external[[\"severity_score\", \"RandomForest\"]]\n",
    "\n",
    "y_train = train[['Survival_30']]\n",
    "y_test = test[['Survival_30']]\n",
    "y_external = external_dataset[['Survival_30']]\n",
    "\n",
    "y_prob = model.predict_proba(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob_ex = model.predict_proba(X_external)\n",
    "y_pred_ex = model.predict(X_external)\n",
    "\n",
    "### validation ###\n",
    "cm = metrics.confusion_matrix([x for [x] in y_test.values.tolist()], y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp+fn+1e-8)\n",
    "specificity = tn / (tn+fp+1e-8)\n",
    "precision = tp/(tp+fp+1e-8)\n",
    "recall = tp/(tp+fn+1e-8)\n",
    "f1 = (2*precision*recall)/(precision+recall+1e-8)\n",
    "\n",
    "print(\"validation\")\n",
    "print('AUC: {:.4f}'.format(metrics.roc_auc_score(y_test, y_prob[:, 1])))\n",
    "print('Acc: {:.4f}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print('sensitivity: {:.4f}'.format(sensitivity))\n",
    "print('specificity: {:.4f}'.format(specificity))\n",
    "print('precision: {:.4f}'.format(precision))\n",
    "print('recall: {:.4f}'.format(recall))\n",
    "print('F1: {:.4f}'.format(f1))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob[:,1])\n",
    "AUC_PR = metrics.auc(tpr, fpr)\n",
    "print('The area under the recall precision curve: {:.4f}'.format(AUC_PR))\n",
    "\n",
    "print(classification_report(y_test.values.tolist(), y_pred, labels=[0,1]))\n",
    "\n",
    "### Confusion Matrics\n",
    "sns.set(font_scale = 1.4) # for label size\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16},fmt='g') # font size\n",
    "plt.show()\n",
    "\n",
    "### Recall Precision curves\n",
    "disp = plot_precision_recall_curve(model, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve')\n",
    "disp.ax_.legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n",
    "### ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_score(y_test, y_prob[:, 1]))\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "### external ###\n",
    "cm = metrics.confusion_matrix([x for [x] in y_external.values.tolist()], y_pred_ex)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp+fn+1e-8)\n",
    "specificity = tn / (tn+fp+1e-8)\n",
    "precision = tp/(tp+fp+1e-8)\n",
    "recall = tp/(tp+fn+1e-8)\n",
    "f1 = (2*precision*recall)/(precision+recall+1e-8)\n",
    "\n",
    "print(\"external test\")\n",
    "print('AUC: {:.4f}'.format(metrics.roc_auc_score(y_external, y_prob_ex[:, 1])))\n",
    "print('Acc: {:.4f}'.format(metrics.accuracy_score(y_external, y_pred_ex)))\n",
    "print('sensitivity: {:.4f}'.format(sensitivity))\n",
    "print('specificity: {:.4f}'.format(specificity))\n",
    "print('precision: {:.4f}'.format(precision))\n",
    "print('recall: {:.4f}'.format(recall))\n",
    "print('F1: {:.4f}'.format(f1))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_external, y_prob_ex[:,1])\n",
    "AUC_PR = metrics.auc(tpr, fpr)\n",
    "print('The area under the recall precision curve: {:.4f}'.format(AUC_PR))\n",
    "\n",
    "print(classification_report(y_external.values.tolist(), y_pred_ex, labels=[0,1]))\n",
    "\n",
    "### Confusion Matrics\n",
    "sns.set(font_scale = 1.4) # for label size\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16},fmt='g') # font size\n",
    "plt.show()\n",
    "\n",
    "### Recall Precision curves\n",
    "disp = plot_precision_recall_curve(model, X_external, y_external)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve')\n",
    "disp.ax_.legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n",
    "### ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_score(y_external, y_prob_ex[:, 1]))\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76401a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.385px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
